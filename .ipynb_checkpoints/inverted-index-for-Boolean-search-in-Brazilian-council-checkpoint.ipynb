{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index for Boolean Search\n",
    "\n",
    "Simple Inverted Index for politic news retrieval having to choose between linked lists or varying arrays for posting.\n",
    "\n",
    ">We ran this search in an important communication media of the brazilian society.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling and design decisions\n",
    "\n",
    "- [x] Make Jupyter cells independent,\n",
    "- [x] Use politic news `id` as ident, and its title as filename,\n",
    "- [x] Include title for retrieval (important),\n",
    "- [x] For unix systems, use suitable filename,\n",
    "- [x] Make corpora compatible to the nltk library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset (extracted from a paid brazilian portal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "politicnews = pd.read_csv('./dataset/news__ptbr.csv', \n",
    "                   names=['title', 'content','id'], \n",
    "                   header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of the dataset\n",
    "```python\n",
    "from googletrans import Translator as t # limit: 15k\n",
    "\n",
    "h = politicnews.head(5); vis = pd.DataFrame(); CHAR=50\n",
    "\n",
    "#Takes up to ~5 seconds for Google Translator response\n",
    "en=lambda x: t().translate(x, \n",
    "                           src=\"pt\", \n",
    "                           dest=\"en\").text[:CHAR]\n",
    "\n",
    "vis[\"title\"] = h[\"title\"].map(en)\n",
    "vis[\"content\"] = h[\"content\"].map(en)\n",
    "vis['id'] = h['id']\n",
    "\n",
    "print(vis) \n",
    "```\n",
    "\n",
    "\n",
    "| title                         | content                            | id   |\n",
    "|-----------------------------  |------------------------------------|------|\n",
    "| 2018 Looking Like 2006        | ...                                  | 1004 |\n",
    "| Social Division May benefit ..   | Brazil defends political scientist | 1032 |\n",
    "| ...                         | ...                              | ...  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details and requirements\n",
    "By having unlimited and measurable alocated size, linked lists was chosen. \n",
    "\n",
    "> 1. Python lists are arrays exponentially over-allocated, surprisingly.\n",
    "\n",
    "This Orderedset offers O(1) for add, remove, and contains. It is coded in C, and its doubly linked nodes [prev, next, value] are lists. Raymond Hettinger’s [OrderedSet](https://orderedset.readthedocs.io/en/latest/) is suitable for fast deletion. It is doubly linked and its implemention can be downloaded [here](https://pypi.org/project/orderedset/#files) and installed by:\n",
    "> pip3 install -r ./dependencies/requirements.txt \n",
    "\n",
    "The corpus can be downloaded by:\n",
    "> nltk.download('all-corpora', quiet=False, raise_on_error=True)\n",
    "\n",
    "See ./dependencies/requirements.py for download only the necessary corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new corpus, suitable for NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code for filename formatting and implementation for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation as punct\n",
    "from nltk.corpus import stopwords as stop\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def create_unix_filename(s, limit, ext):\n",
    "    name = [lemma for lemma in word_tokenize(s[:limit].lower()) \n",
    "            if lemma not in punct and stop.words('english')\n",
    "           if len(lemma)>3] #removes \"'s\", \"in\", \"the\", etc\n",
    "    return ('-'.join(name)+ext)\n",
    "\n",
    "def create_corpus(folder, dataframe):\n",
    "    for i, row in dataframe.iterrows():\n",
    "        ident=row['id']; title=row['title']; content=row['content']\n",
    "        \n",
    "        limit=60 #character limit for fileids (less than or equal 255 characters for unix)\n",
    "        article=str(ident)+'_'+create_unix_filename(title,limit,'.txt') #translate title for results below\n",
    "\n",
    "        corpus=open(folder+'/'+article,'a')\n",
    "        corpus.write(' [ '+title+' ] '+content)\n",
    "        corpus.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make folder for storing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news__ptbr.csv\n",
      "portuguese_council\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "#!/bin/sh\n",
    "\n",
    "dir=./dataset\n",
    "mkdir -p $dir/portuguese_council\n",
    "ls $dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1 s ± 123 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "create_corpus('./dataset/portuguese_council/', politicnews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate corpus reader for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7643"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "\n",
    "corpus=PlaintextCorpusReader('./dataset/portuguese_council/', r'.*\\.txt')\n",
    "\n",
    "len(corpus.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 15 fileids (troublesome \" 's \" already removed, and translation altogether):\n",
    "<pre>├── 1002_aécio-is-the-best-voted-tucano-in-sp.txt\n",
    "├── 1004_2018-looking-like-2006.txt\n",
    "├── 1032_social-division-may-benefit-brazil-defends-political-scientist.txt\n",
    "├── 1046_aécio-will-accompany-verification-at-her-sister-&apos;s-house-in-bh.txt\n",
    "├── 1064_aécio-says-he-expects-youssef-to-tell-everything-he-knows.txt\n",
    "├── 1079_aécio-if-elected-first-mission-is-to-unify-the-country.txt\n",
    "├── 1094_aécio-votes-in-bh-and-talks-about-unifying-the-country.txt\n",
    "├── 1114_cid-&apos;s-godson-turns-the-game-over-the-favorite.txt\n",
    "├── 1128_lobby-agenda-is-waiting-for-the-president.txt\n",
    "├── 1131_the-election-in-which-it-is-urgent-to-have.txt\n",
    "├── 1133_aecio-reacts-and-also-uses-his-toolbox.txt\n",
    "├── 1151_aécio-neves-says-he-will-fire-petrobras-board-if-elected.txt\n",
    "├── 1160_ácio-accuses-pt-of-terrorism-dilma-highlights-pronatec.txt\n",
    "├── 1186_aécio-and-dilma-arrive-at-the-last-debate-without-trying-cup.txt\n",
    "├── 1208_agreement-is-initial-act-of-political-reform-says-janot.txt\n",
    "(...)</pre>\n",
    "\n",
    "Use this command for cleansing:\n",
    "\n",
    "```sh\n",
    "find ./portuguese_council/ -maxdepth 1 -name \"*.txt\" -print0 | xargs -0 rm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The inverted index for indexing news (or articles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, collections \n",
    "from orderedset import OrderedSet\n",
    "\n",
    "def inverted_index(corpus):\n",
    "    i = collections.defaultdict(OrderedSet)\n",
    "    for filename in corpus.fileids():\n",
    "        for term in corpus.words(filename):\n",
    "            i[term.lower()].add(filename)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for testing the inverted index (only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This do not replace boolean syntactic analyzers.\n",
    "\n",
    "def search(i, query):\n",
    "    \"\"\"\n",
    "    :param i: inverted index, case insenstive\n",
    "    :param query: user input, patterns:\n",
    "       1-term: if found, a term is returned;\n",
    "       term1 AND term2: all query terms;\n",
    "       term1 OR term2: any of the query terms.\n",
    "    \"\"\"\n",
    "    t=lambda s: s.lower().strip()   \n",
    "    if ' OR ' in query:\n",
    "        term = query.split(' OR ')\n",
    "        return i[t(term[0])].union(i[t(term[1])]) \n",
    "    elif ' AND ' in query:\n",
    "        term = query.split(' AND ')\n",
    "        return i[t(term[0])].intersection(i[t(term[1])])\n",
    "    else:\n",
    "        return i[t(query)]  #case insensitive  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.1 s ± 496 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Generating the indexes for all news\n",
    "i = inverted_index(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running again for having the index to perform the tests below\n",
    "i = inverted_index(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from orderedset import OrderedSet\n",
    "\n",
    "# Lowercased for indexing check (see inverted_index above, and search below)\n",
    "c=i[\"campina\"]\n",
    "g=i['grande']\n",
    "\n",
    "len(c.intersection(g)) # Must be 12 occurrences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing (case insensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(search(i,\"Belo AND Horizonte\")) ==  len(search(i,\"belo AND horizonte\"))) # Case-insensitive assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(search(i,\" latin \")) == len(search(i,\"Latin\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(search(i, \"debate OR presidencial\")) == 1772 # Include titles.\n",
    "\n",
    "assert len(search(i, \"debate AND presidencial\")) == 201 \n",
    "\n",
    "assert len(search(i, \"presidenciáveis OR corruptos\")) == 164\n",
    "\n",
    "assert len(search(i, \"presidenciáveis AND corruptos\")) == 0\n",
    "\n",
    "assert len(search(i, \"Belo OR Horizonte\")) == 331\n",
    "\n",
    "assert len(search(i, \"Belo AND Horizonte\")) == 242"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occurences of word \"Basque\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i, 'Basque OR Vascos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i, 'Basques')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occurrences of \"Latin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, '                 Latin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'Latino'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using orderedset in NLTK corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "3.61 s ± 20.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Bonus Exercise: Using orderedset, compute hit list for ((paris AND NOT france) OR lear)\n",
    "def dsearch(inverted_index):\n",
    "    files_with_paris = inverted_index['Paris']\n",
    "    files_with_france = inverted_index['France']\n",
    "    files_with_lear = inverted_index['Lear']\n",
    "    return ((files_with_paris.difference(files_with_france)).union(files_with_lear))\n",
    "\n",
    "inltk = inverted_index(nltk.corpus.gutenberg) # Creates the corpus\n",
    "\n",
    "print(dsearch(inltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case-sensitive result\n",
    "OrderedSet(['whitman-leaves.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'Italiano'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'falso OR falsa')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'Italiano OR Italiana')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'polonês OR polish')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'insanidade OR sanidade')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'Holandês')) #case insenstive ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i, 'England')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i, 'Reino Unido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i,'Grã-Bretanha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i,'Espanha'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing features and improvements\n",
    "- [ ] Check PEP8 convention\n",
    "- [ ] Reduce the number of syscalls by reducing and unifying achieves\n",
    "- [x] Reduce filenames by removing words less than or equal 3 characters\n",
    "- [ ] Improve requirements\n",
    "\n",
    "> Although provided by the NLTK library, this document do not apply \"cleansing\" of the portuguese. For example, in exclusion of stoping words, synonyms words, and other usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conslusions\n",
    "\n",
    "Boolean search is simplistic yet powerful approach for retrieval. The case-sensitive model achieved all indexation in ~8s, against ~16s of its oppose, being possible to search for \"Englishman\" which returned no results, its oppose returned 30 occurences of unrelated subjects.\n",
    "\n",
    "By having unlimited and measurable alocated size, linked lists was chosen. The ordered set offered constant time complexity, O(1) for add, remove, and contains. Although other data structures would benefit specific scenarios, excellent results was obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Thanks\n",
    "- Leandro Balby for have required the exercise and provided the dataset.\n",
    "- Joe James for have provided an overview of the NLTK library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <footer>\n",
    "  <p>Suggestions, and more. Contact information at <a href=\"mailto:aabarbosa.cs@gmail.com\">\n",
    "  gmail</a>.</p>\n",
    "</footer> \n",
    "\n",
    "<style>\n",
    "footer {\n",
    "  display: block;\n",
    "}\n",
    "    </style>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
