{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index for Boolean Search\n",
    "\n",
    "Simple Inverted Index for news retrieval (having to choose between linked lists or varying arrays for posting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling and design decisions\n",
    "\n",
    "- [x] Make cells independent one another\n",
    "- [x] Use ids as indexes and in filenames as ident\n",
    "- [x] Make title important for retrieval\n",
    "- [x] Use suitable filename for unix systems\n",
    "- [x] Make corpora compatible to the nltk library\n",
    "\n",
    "\n",
    "\n",
    "> Although provided by the NLTK library, this document do not apply \"cleansing\" of the portuguese. For example, in exclusion of stoping words, synonyms words, and other usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset (extracted from a paid brazilian portal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "politicnews = pd.read_csv('./dataset/news__ptbr.csv', \n",
    "                   names=['title', 'content','id'], \n",
    "                   header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of the dataset\n",
    "```python\n",
    "from googletrans import Translator as t # limit: 15k\n",
    "\n",
    "h = politicnews.head(5); vis = pd.DataFrame(); CHAR=50\n",
    "\n",
    "#Takes up to ~5 seconds for Google Translator response\n",
    "en=lambda x: t().translate(x, \n",
    "                           src=\"pt\", \n",
    "                           dest=\"en\").text[:CHAR]\n",
    "\n",
    "vis[\"title\"] = h[\"title\"].map(en)\n",
    "vis[\"content\"] = h[\"content\"].map(en)\n",
    "vis['id'] = h['id']\n",
    "\n",
    "print(vis) \n",
    "```\n",
    "\n",
    "\n",
    "| title                         | content                            | id   |\n",
    "|-----------------------------  |------------------------------------|------|\n",
    "| 2018 Looking Like 2006        | ...                                  | 1004 |\n",
    "| Social Division May benefit ..   | Brazil defends political scientist | 1032 |\n",
    "| ...                         | ...                              | ...  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details and requirements\n",
    "By having unlimited and measurable alocated size, linked lists was chosen. Raymond Hettinger’s [OrderedSet](https://orderedset.readthedocs.io/en/latest/) is suitable for fast deletion. It is doubly linked and its implemention can be downloaded [here](https://pypi.org/project/orderedset/#files) and be installed by:\n",
    "```terminal\n",
    "pip3 install orderedset\n",
    "``` \n",
    "> Python lists are arrays exponentially over-allocated, surprisingly.\n",
    "\n",
    "This Orderedset offers O(1) for add, remove, and contains. It is coded in C, and its doubly linked nodes [prev, next, value] are lists. Other data structures would benefit specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new corpus, suitable for NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code for filename formating\n",
    "- The corpus implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation as punct\n",
    "from nltk.corpus import stopwords as stop\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def create_unix_filename(s, limit, ext):\n",
    "    name = [lemma for lemma in word_tokenize(s[:limit].lower()) \n",
    "            if lemma not in punct and stop.words('english')\n",
    "           if len(lemma)>3] #removes \"'s\", \"in\", \"the\", etc\n",
    "    return ('-'.join(name)+ext)\n",
    "\n",
    "def create_corpus(folder, dataframe):\n",
    "    for i, row in dataframe.iterrows():\n",
    "        ident=row['id']; title=row['title']\n",
    "        content=row['content']\n",
    "        \n",
    "        limit=60 #character limit for fileids (less than or equal 255 characters for unix)\n",
    "        article=str(ident)+'_'+create_unix_filename(title,limit,'.txt') #translate title for results below\n",
    "\n",
    "        corpus=open(folder+'/'+article,'a')\n",
    "        corpus.write(' [ '+title+' ] '+content)\n",
    "        corpus.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make folder for storing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news__ptbr.csv\n",
      "portuguese_council\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "#!/bin/sh\n",
    "\n",
    "dir=./dataset\n",
    "mkdir -p $dir/portuguese_council\n",
    "ls $dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 8.43 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "create_corpus('./dataset/portuguese_council/', politicnews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate corpus reader for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7643"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "\n",
    "corpus=PlaintextCorpusReader('./dataset/portuguese_council/', r'.*\\.txt')\n",
    "\n",
    "len(corpus.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 15 fileids (troublesome \" 's \" already removed and translation altogether):\n",
    "<pre>├── 1002_aécio-is-the-best-voted-tucano-in-sp.txt\n",
    "├── 1004_2018-looking-like-2006.txt\n",
    "├── 1032_social-division-may-benefit-brazil-defends-political-scientist.txt\n",
    "├── 1046_aécio-will-accompany-verification-at-her-sister-&apos;s-house-in-bh.txt\n",
    "├── 1064_aécio-says-he-expects-youssef-to-tell-everything-he-knows.txt\n",
    "├── 1079_aécio-if-elected-first-mission-is-to-unify-the-country.txt\n",
    "├── 1094_aécio-votes-in-bh-and-talks-about-unifying-the-country.txt\n",
    "├── 1114_cid-&apos;s-godson-turns-the-game-over-the-favorite.txt\n",
    "├── 1128_lobby-agenda-is-waiting-for-the-president.txt\n",
    "├── 1131_the-election-in-which-it-is-urgent-to-have.txt\n",
    "├── 1133_aecio-reacts-and-also-uses-his-toolbox.txt\n",
    "├── 1151_aécio-neves-says-he-will-fire-petrobras-board-if-elected.txt\n",
    "├── 1160_ácio-accuses-pt-of-terrorism-dilma-highlights-pronatec.txt\n",
    "├── 1186_aécio-and-dilma-arrive-at-the-last-debate-without-trying-cup.txt\n",
    "├── 1208_agreement-is-initial-act-of-political-reform-says-janot.txt\n",
    "(...)</pre>\n",
    "\n",
    "Use this command for cleansing:\n",
    "\n",
    "```sh\n",
    "find ./portuguese_council/ -maxdepth 1 -name \"*.txt\" -print0 | xargs -0 rm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The inverted index for indexing news (or articles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, collections \n",
    "from orderedset import OrderedSet\n",
    "\n",
    "def inverted_index(corpus):\n",
    "    i = collections.defaultdict(OrderedSet)\n",
    "    for filename in corpus.fileids():\n",
    "        for term in corpus.words(filename):\n",
    "            i[term.lower()].add(filename)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for testing the inverted index (only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This do not replace boolean syntactic analyzers.\n",
    "\n",
    "def search(i, query):\n",
    "    \"\"\"\n",
    "    :param i: inverted index, case insenstive\n",
    "    :param query: user input, patterns:\n",
    "       1-term: if found, a term is returned;\n",
    "       term1 AND term2: all query terms;\n",
    "       term1 OR term2: any of the query terms.\n",
    "    \"\"\"\n",
    "    t=lambda s: s.lower().strip()   \n",
    "    if ' OR ' in query:\n",
    "        term = query.split(' OR ')\n",
    "        return i[t(term[0])].union(i[t(term[1])]) \n",
    "    elif ' AND ' in query:\n",
    "        term = query.split(' AND ')\n",
    "        return i[t(term[0])].intersection(i[t(term[1])])\n",
    "    else:\n",
    "        return i[t(query)]  #case insensitive  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 16.5 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Generating the indexes for all news\n",
    "i = inverted_index(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running again for having the index to perform the tests below\n",
    "i = inverted_index(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (case sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from orderedset import OrderedSet\n",
    "\n",
    "c=i[\"Campina\"]\n",
    "g=i['Grande']\n",
    "\n",
    "len(c.intersection(g)) # Must be 12 occurrences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (case insensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert (len(search(i,\"Belo AND Horizonte\")) ==  len(search(i,\"belo AND horizonte\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(search(i,\" latin \")) == len(search(i,\"Latin\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occurences of word \"Basque\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i, 'Basque OR Vascos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i, 'Basques')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occurrences of \"Latin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, '                 Latin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'Latino'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'Italiano'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'falso OR falsa')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'Italiano OR Italiana')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using orderedset in NLTK corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "OrderedSet()\n",
      "1 loop, best of 5: 2.92 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Bonus Exercise: Using orderedset, compute hit list for ((paris AND NOT france) OR lear)\n",
    "def dsearch(inverted_index):\n",
    "    files_with_paris = inverted_index['Paris']\n",
    "    files_with_france = inverted_index['France']\n",
    "    files_with_lear = inverted_index['Lear']\n",
    "    return ((files_with_paris.difference(files_with_france)).union(files_with_lear))\n",
    "\n",
    "i = inverted_index(nltk.corpus.gutenberg)\n",
    "\n",
    "print(dsearch(i))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OrderedSet(['whitman-leaves.txt']) #Case sensitive\n",
    "OrderedSet(['whitman-leaves.txt'])\n",
    "OrderedSet(['whitman-leaves.txt'])\n",
    "OrderedSet(['whitman-leaves.txt'])\n",
    "OrderedSet(['whitman-leaves.txt'])\n",
    "1 loop, best of 5: missing s per loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conslusions\n",
    "\n",
    "Boolean search is a simplistic approach yet powerful. The case-sensitive approach showed faster indexation, ~8s against ~16s of its oppose. It was also possible to search for \"Englishman\", which returned no results, while its opposed returned 30 occurences of unrelated subjects. \n",
    "\n",
    "\n",
    ">We ran this search in an important communication media of the brazilian society.\n",
    "\n",
    "\n",
    "It follows the search for \"Polish\", although its existance in the culturalization processs for finding any result of this word. #all case-insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'polonês OR polish')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'insanidade OR sanidade')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i, 'Holandês')) #case insenstive ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i, 'England')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i, 'Reino Unido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(i,'Grã-Bretanha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search(i,'Espanha'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing features and improvements\n",
    "\n",
    "- [ ] Reduce the number of syscalls by reducing files and unifying achieves\n",
    "- [x] Reduce filenames by removing words with size less than or equal 3 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Thanks\n",
    "- Leandro Balby for have required and provided the dataset.\n",
    "- Joe James for have provided an overview of NLTK library.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
